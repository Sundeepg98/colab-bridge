{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Advanced Processing Server for AI Platform\n",
    "\n",
    "This notebook provides GPU-accelerated processing capabilities for your AI platform.\n",
    "\n",
    "## Features:\n",
    "- Multi-modal AI processing (text, image, video)\n",
    "- GPU-accelerated embeddings\n",
    "- Neural search capabilities\n",
    "- Style transfer and image generation\n",
    "- Batch processing for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"üîç Checking available resources...\\n\")\n",
    "\n",
    "# PyTorch GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ PyTorch GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå No PyTorch GPU found\")\n",
    "\n",
    "# TensorFlow GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\n‚úÖ TensorFlow GPUs: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   {gpu}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No TensorFlow GPU found\")\n",
    "\n",
    "# Check TPU\n",
    "try:\n",
    "    import jax\n",
    "    print(f\"\\nüîß JAX devices: {jax.devices()}\")\n",
    "except:\n",
    "    print(\"\\n‚ÑπÔ∏è JAX not available (TPU check skipped)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers diffusers accelerate\n",
    "!pip install -q sentence-transformers faiss-gpu\n",
    "!pip install -q opencv-python-headless pillow\n",
    "!pip install -q fastapi uvicorn pyngrok\n",
    "!pip install -q einops xformers\n",
    "!pip install -q torchvision torchaudio\n",
    "\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "processing-engine"
   },
   "source": [
    "## 2. Advanced Processing Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "processing-core"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import hashlib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AdvancedProcessor:\n",
    "    \"\"\"Main processing engine with GPU acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f\"Processor initialized on {self.device}\")\n",
    "        \n",
    "        # Model cache\n",
    "        self.models = {}\n",
    "        self.model_loading_lock = asyncio.Lock()\n",
    "        \n",
    "        # Initialize models lazily\n",
    "        self.text_model = None\n",
    "        self.image_model = None\n",
    "        self.embedding_model = None\n",
    "        self.style_transfer_model = None\n",
    "        \n",
    "        # Processing stats\n",
    "        self.stats = {\n",
    "            'tasks_processed': 0,\n",
    "            'total_processing_time': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "    \n",
    "    async def initialize_text_model(self):\n",
    "        \"\"\"Initialize text generation model\"\"\"\n",
    "        if self.text_model is None:\n",
    "            async with self.model_loading_lock:\n",
    "                if self.text_model is None:  # Double check\n",
    "                    logger.info(\"Loading text generation model...\")\n",
    "                    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "                    \n",
    "                    model_name = \"microsoft/DialoGPT-medium\"\n",
    "                    self.text_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                    self.text_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "                    self.text_model.to(self.device)\n",
    "                    self.text_model.eval()\n",
    "                    \n",
    "                    # Add padding token\n",
    "                    self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "                    \n",
    "                    logger.info(\"Text model loaded successfully\")\n",
    "    \n",
    "    async def initialize_image_model(self):\n",
    "        \"\"\"Initialize image generation model\"\"\"\n",
    "        if self.image_model is None:\n",
    "            async with self.model_loading_lock:\n",
    "                if self.image_model is None:\n",
    "                    logger.info(\"Loading image generation model...\")\n",
    "                    from diffusers import StableDiffusionPipeline\n",
    "                    \n",
    "                    # Use smaller model for Colab\n",
    "                    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "                    self.image_model = StableDiffusionPipeline.from_pretrained(\n",
    "                        model_id,\n",
    "                        torch_dtype=torch.float16,\n",
    "                        revision=\"fp16\",\n",
    "                        use_auth_token=False\n",
    "                    )\n",
    "                    self.image_model = self.image_model.to(self.device)\n",
    "                    \n",
    "                    # Enable memory efficient attention\n",
    "                    try:\n",
    "                        self.image_model.enable_xformers_memory_efficient_attention()\n",
    "                    except:\n",
    "                        logger.warning(\"xformers not available, using default attention\")\n",
    "                    \n",
    "                    logger.info(\"Image model loaded successfully\")\n",
    "    \n",
    "    async def initialize_embedding_model(self):\n",
    "        \"\"\"Initialize embedding model\"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            async with self.model_loading_lock:\n",
    "                if self.embedding_model is None:\n",
    "                    logger.info(\"Loading embedding model...\")\n",
    "                    from sentence_transformers import SentenceTransformer\n",
    "                    \n",
    "                    self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "                    self.embedding_model.to(self.device)\n",
    "                    \n",
    "                    logger.info(\"Embedding model loaded successfully\")\n",
    "    \n",
    "    async def process_text(self, text: str, model: str = \"default\", \n",
    "                          parameters: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process text generation request\"\"\"\n",
    "        await self.initialize_text_model()\n",
    "        \n",
    "        parameters = parameters or {}\n",
    "        max_length = parameters.get('max_length', 100)\n",
    "        temperature = parameters.get('temperature', 0.7)\n",
    "        top_p = parameters.get('top_p', 0.9)\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.text_tokenizer.encode(text, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_model.generate(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.text_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.text_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'generated_text': generated_text,\n",
    "            'model': model,\n",
    "            'parameters': parameters\n",
    "        }\n",
    "    \n",
    "    async def generate_image(self, prompt: str, style: Optional[str] = None,\n",
    "                           size: str = \"512x512\", **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generate image from text prompt\"\"\"\n",
    "        await self.initialize_image_model()\n",
    "        \n",
    "        # Parse size\n",
    "        width, height = map(int, size.split('x'))\n",
    "        \n",
    "        # Apply style if provided\n",
    "        if style:\n",
    "            style_prompts = {\n",
    "                'anime': ', anime style, manga, japanese animation',\n",
    "                'realistic': ', photorealistic, high detail, professional photography',\n",
    "                'artistic': ', artistic, oil painting, masterpiece',\n",
    "                'cyberpunk': ', cyberpunk style, neon, futuristic',\n",
    "                'fantasy': ', fantasy art, magical, ethereal'\n",
    "            }\n",
    "            prompt += style_prompts.get(style, '')\n",
    "        \n",
    "        # Generate image\n",
    "        with torch.no_grad():\n",
    "            image = self.image_model(\n",
    "                prompt,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_inference_steps=kwargs.get('steps', 50),\n",
    "                guidance_scale=kwargs.get('guidance_scale', 7.5)\n",
    "            ).images[0]\n",
    "        \n",
    "        # Convert to base64\n",
    "        buffered = io.BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "        \n",
    "        return {\n",
    "            'image': img_base64,\n",
    "            'prompt': prompt,\n",
    "            'size': size,\n",
    "            'style': style\n",
    "        }\n",
    "    \n",
    "    async def generate_embeddings(self, texts: List[str], \n",
    "                                model: str = \"sentence-transformers\") -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for texts\"\"\"\n",
    "        await self.initialize_embedding_model()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.embedding_model.encode(\n",
    "                texts,\n",
    "                convert_to_tensor=True,\n",
    "                device=self.device,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        \n",
    "        return embeddings.cpu().numpy()\n",
    "    \n",
    "    async def neural_search(self, query: str, corpus: List[str], \n",
    "                          top_k: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Perform neural search\"\"\"\n",
    "        await self.initialize_embedding_model()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        query_embedding = await self.generate_embeddings([query])\n",
    "        corpus_embeddings = await self.generate_embeddings(corpus)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "        \n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': corpus[idx],\n",
    "                'score': float(similarities[idx]),\n",
    "                'index': int(idx)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    async def batch_process(self, items: List[Dict], processor: str,\n",
    "                          batch_size: int = 32) -> List[Dict]:\n",
    "        \"\"\"Process items in batch\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(items), batch_size):\n",
    "            batch = items[i:i + batch_size]\n",
    "            \n",
    "            if processor == \"embeddings\":\n",
    "                texts = [item.get('text', '') for item in batch]\n",
    "                embeddings = await self.generate_embeddings(texts)\n",
    "                \n",
    "                for j, embedding in enumerate(embeddings):\n",
    "                    results.append({\n",
    "                        'item_id': batch[j].get('id', i + j),\n",
    "                        'embedding': embedding.tolist()\n",
    "                    })\n",
    "            \n",
    "            elif processor == \"text_analysis\":\n",
    "                for item in batch:\n",
    "                    text = item.get('text', '')\n",
    "                    # Simple analysis - can be extended\n",
    "                    results.append({\n",
    "                        'item_id': item.get('id'),\n",
    "                        'length': len(text.split()),\n",
    "                        'sentiment': self._analyze_sentiment(text)\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _analyze_sentiment(self, text: str) -> str:\n",
    "        \"\"\"Simple sentiment analysis\"\"\"\n",
    "        positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'love'}\n",
    "        negative_words = {'bad', 'terrible', 'awful', 'hate', 'poor', 'worst'}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        pos_count = sum(1 for word in positive_words if word in text_lower)\n",
    "        neg_count = sum(1 for word in negative_words if word in text_lower)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return 'positive'\n",
    "        elif neg_count > pos_count:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        return {\n",
    "            'device': str(self.device),\n",
    "            'models_loaded': {\n",
    "                'text': self.text_model is not None,\n",
    "                'image': self.image_model is not None,\n",
    "                'embedding': self.embedding_model is not None\n",
    "            },\n",
    "            'stats': self.stats\n",
    "        }\n",
    "\n",
    "# Initialize processor\n",
    "processor = AdvancedProcessor()\n",
    "print(\"‚úÖ Advanced processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api-server"
   },
   "source": [
    "## 3. FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fastapi-setup"
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "import uvicorn\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Allow nested asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API Models\n",
    "class TextProcessRequest(BaseModel):\n",
    "    task_id: str\n",
    "    task_type: str\n",
    "    payload: Dict[str, Any]\n",
    "    priority: int = 5\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    status: str = \"healthy\"\n",
    "    load: float\n",
    "    memory_available_gb: float\n",
    "    models_loaded: Dict[str, bool]\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"Colab Advanced Processing Server\", version=\"1.0.0\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"Colab Advanced Processing Server\",\n",
    "        \"capabilities\": [\n",
    "            \"text_generation\",\n",
    "            \"image_generation\",\n",
    "            \"embeddings\",\n",
    "            \"neural_search\",\n",
    "            \"batch_processing\"\n",
    "        ],\n",
    "        \"gpu_available\": torch.cuda.is_available()\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    # Calculate load (simple estimation)\n",
    "    if torch.cuda.is_available():\n",
    "        load = torch.cuda.utilization() / 100.0\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        memory_available = memory_total - memory_used\n",
    "    else:\n",
    "        load = 0.3  # Dummy value for CPU\n",
    "        memory_available = 12.0\n",
    "    \n",
    "    return HealthResponse(\n",
    "        load=load,\n",
    "        memory_available_gb=memory_available,\n",
    "        models_loaded={\n",
    "            'text': processor.text_model is not None,\n",
    "            'image': processor.image_model is not None,\n",
    "            'embedding': processor.embedding_model is not None\n",
    "        }\n",
    "    )\n",
    "\n",
    "@app.post(\"/process\")\n",
    "async def process_task(request: TextProcessRequest):\n",
    "    \"\"\"Main processing endpoint\"\"\"\n",
    "    try:\n",
    "        task_type = request.task_type\n",
    "        payload = request.payload\n",
    "        \n",
    "        result = None\n",
    "        \n",
    "        if task_type == \"text_generation\":\n",
    "            result = await processor.process_text(\n",
    "                payload.get('text', ''),\n",
    "                payload.get('model', 'default'),\n",
    "                payload.get('parameters', {})\n",
    "            )\n",
    "        \n",
    "        elif task_type == \"image_generation\":\n",
    "            result = await processor.generate_image(\n",
    "                payload.get('prompt', ''),\n",
    "                payload.get('style'),\n",
    "                payload.get('size', '512x512'),\n",
    "                **{k: v for k, v in payload.items() if k not in ['prompt', 'style', 'size']}\n",
    "            )\n",
    "        \n",
    "        elif task_type == \"embeddings\":\n",
    "            embeddings = await processor.generate_embeddings(\n",
    "                payload.get('texts', []),\n",
    "                payload.get('model', 'sentence-transformers')\n",
    "            )\n",
    "            result = {'embeddings': embeddings.tolist()}\n",
    "        \n",
    "        elif task_type == \"neural_search\":\n",
    "            results = await processor.neural_search(\n",
    "                payload.get('query', ''),\n",
    "                payload.get('corpus', []),\n",
    "                payload.get('top_k', 10)\n",
    "            )\n",
    "            result = {'results': results}\n",
    "        \n",
    "        elif task_type == \"batch_processing\":\n",
    "            processed = await processor.batch_process(\n",
    "                payload.get('items', []),\n",
    "                payload.get('processor', ''),\n",
    "                payload.get('batch_size', 32)\n",
    "            )\n",
    "            result = {'processed_items': processed}\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "        \n",
    "        # Update stats\n",
    "        processor.stats['tasks_processed'] += 1\n",
    "        \n",
    "        return {\n",
    "            'task_id': request.task_id,\n",
    "            'status': 'completed',\n",
    "            'result': result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        processor.stats['errors'] += 1\n",
    "        logger.error(f\"Processing error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/stats\")\n",
    "async def get_stats():\n",
    "    \"\"\"Get processing statistics\"\"\"\n",
    "    return processor.get_stats()\n",
    "\n",
    "# Start server function\n",
    "def start_server(port=8080):\n",
    "    \"\"\"Start the FastAPI server\"\"\"\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "print(\"‚úÖ FastAPI server configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngrok-setup"
   },
   "source": [
    "## 4. Deploy with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy"
   },
   "outputs": [],
   "source": [
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_server, args=(8080,), daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Give server time to start\n",
    "time.sleep(3)\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(8080)\n",
    "print(f\"\\nüåê Processing Server URL: {public_url}\")\n",
    "print(f\"\\nüìã Add this to your Flask app as a Colab resource:\")\n",
    "print(f\"\\ninstance_id: 'colab_gpu_1'\")\n",
    "print(f\"url: '{public_url}'\")\n",
    "print(f\"capabilities: ['text_generation', 'image_generation', 'embeddings', 'neural_search']\")\n",
    "print(f\"gpu_type: '{torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}'\")\n",
    "\n",
    "# Test the server\n",
    "import requests\n",
    "test_response = requests.get(f\"{public_url}/health\")\n",
    "print(f\"\\nüß™ Health Check: {test_response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-section"
   },
   "source": [
    "## 5. Test Processing Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-processing"
   },
   "outputs": [],
   "source": [
    "# Test various processing capabilities\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"üß™ Testing Processing Capabilities\\n\")\n",
    "\n",
    "# Test 1: Text Generation\n",
    "print(\"1Ô∏è‚É£ Testing Text Generation...\")\n",
    "text_request = {\n",
    "    \"task_id\": \"test_text_001\",\n",
    "    \"task_type\": \"text_generation\",\n",
    "    \"payload\": {\n",
    "        \"text\": \"The future of AI is\",\n",
    "        \"parameters\": {\n",
    "            \"max_length\": 50,\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{public_url}/process\", json=text_request)\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"‚úÖ Generated: {result['result']['generated_text'][:100]}...\\n\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.text}\\n\")\n",
    "\n",
    "# Test 2: Embeddings\n",
    "print(\"2Ô∏è‚É£ Testing Embeddings...\")\n",
    "embedding_request = {\n",
    "    \"task_id\": \"test_embed_001\",\n",
    "    \"task_type\": \"embeddings\",\n",
    "    \"payload\": {\n",
    "        \"texts\": [\n",
    "            \"Machine learning is amazing\",\n",
    "            \"AI will transform the world\",\n",
    "            \"Deep learning models are powerful\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{public_url}/process\", json=embedding_request)\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    embeddings = result['result']['embeddings']\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
    "    print(f\"   Embedding shape: {len(embeddings[0])} dimensions\\n\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.text}\\n\")\n",
    "\n",
    "# Test 3: Neural Search\n",
    "print(\"3Ô∏è‚É£ Testing Neural Search...\")\n",
    "search_request = {\n",
    "    \"task_id\": \"test_search_001\",\n",
    "    \"task_type\": \"neural_search\",\n",
    "    \"payload\": {\n",
    "        \"query\": \"artificial intelligence applications\",\n",
    "        \"corpus\": [\n",
    "            \"AI is used in healthcare for diagnosis\",\n",
    "            \"Machine learning powers recommendation systems\",\n",
    "            \"Computer vision enables autonomous vehicles\",\n",
    "            \"Natural language processing helps chatbots\",\n",
    "            \"Deep learning improves image recognition\"\n",
    "        ],\n",
    "        \"top_k\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{public_url}/process\", json=search_request)\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    search_results = result['result']['results']\n",
    "    print(\"‚úÖ Search Results:\")\n",
    "    for i, res in enumerate(search_results):\n",
    "        print(f\"   {i+1}. {res['text']} (score: {res['score']:.3f})\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.text}\")\n",
    "\n",
    "print(\"\\n‚úÖ All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitoring"
   },
   "source": [
    "## 6. Resource Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor-resources"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "def plot_resource_usage():\n",
    "    \"\"\"Plot current resource usage\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Colab Resource Usage', fontsize=16)\n",
    "    \n",
    "    # CPU Usage\n",
    "    ax1 = axes[0, 0]\n",
    "    cpu_percent = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    ax1.bar(range(len(cpu_percent)), cpu_percent)\n",
    "    ax1.set_title('CPU Usage by Core')\n",
    "    ax1.set_ylabel('Usage %')\n",
    "    ax1.set_xlabel('CPU Core')\n",
    "    \n",
    "    # Memory Usage\n",
    "    ax2 = axes[0, 1]\n",
    "    memory = psutil.virtual_memory()\n",
    "    labels = ['Used', 'Available']\n",
    "    sizes = [memory.used / 1e9, memory.available / 1e9]\n",
    "    ax2.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "    ax2.set_title(f'Memory Usage (Total: {memory.total / 1e9:.1f} GB)')\n",
    "    \n",
    "    # GPU Usage\n",
    "    ax3 = axes[1, 0]\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_data = {\n",
    "            'GPU Load': gpu.load * 100,\n",
    "            'Memory Used': (gpu.memoryUsed / gpu.memoryTotal) * 100\n",
    "        }\n",
    "        ax3.bar(gpu_data.keys(), gpu_data.values())\n",
    "        ax3.set_title(f'GPU Usage ({gpu.name})')\n",
    "        ax3.set_ylabel('Usage %')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No GPU Available', ha='center', va='center')\n",
    "        ax3.set_title('GPU Usage')\n",
    "    \n",
    "    # Processing Stats\n",
    "    ax4 = axes[1, 1]\n",
    "    stats = processor.get_stats()['stats']\n",
    "    stat_labels = list(stats.keys())\n",
    "    stat_values = list(stats.values())\n",
    "    ax4.bar(stat_labels, stat_values)\n",
    "    ax4.set_title('Processing Statistics')\n",
    "    ax4.set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot current usage\n",
    "plot_resource_usage()\n",
    "\n",
    "# Show current stats\n",
    "print(\"\\nüìä Current Processing Stats:\")\n",
    "stats = processor.get_stats()\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keep-alive"
   },
   "source": [
    "## 7. Keep Server Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keep-running"
   },
   "outputs": [],
   "source": [
    "# Keep the server running\n",
    "print(\"üöÄ Server is running!\")\n",
    "print(f\"\\nüìã Server URL: {public_url}\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server\")\n",
    "print(\"\\nüí° The server will automatically handle requests from your Flask app\")\n",
    "\n",
    "# Monitor loop\n",
    "while True:\n",
    "    time.sleep(60)  # Check every minute\n",
    "    \n",
    "    # Check health\n",
    "    try:\n",
    "        health = requests.get(f\"{public_url}/health\", timeout=5)\n",
    "        if health.status_code == 200:\n",
    "            data = health.json()\n",
    "            print(f\"\\r‚úÖ Server healthy | Load: {data['load']:.1%} | Memory: {data['memory_available_gb']:.1f}GB | Tasks: {processor.stats['tasks_processed']}\", end='')\n",
    "    except:\n",
    "        print(\"\\r‚ùå Server health check failed\", end='')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}