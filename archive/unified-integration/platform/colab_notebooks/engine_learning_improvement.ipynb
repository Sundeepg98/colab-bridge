{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "learning-header"
   },
   "source": [
    "# üß† AI Platform Engine Learning Enhancement\n",
    "\n",
    "This notebook enhances the learning capabilities of your AI platform using Google Colab's GPU/TPU resources.\n",
    "\n",
    "## Features:\n",
    "- Pattern recognition enhancement\n",
    "- User behavior analysis\n",
    "- Model fine-tuning\n",
    "- Real-time learning optimization\n",
    "- Multi-modal learning integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. Environment Setup & GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu-check"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and setup\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Check for GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print(f'‚úÖ GPU found: {tf.test.gpu_device_name()}')\n",
    "else:\n",
    "    print('‚ùå No GPU found. Using CPU.')\n",
    "\n",
    "# PyTorch GPU check\n",
    "print(f'\\nüî• PyTorch GPU: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q scikit-learn pandas matplotlib seaborn\n",
    "!pip install -q wandb  # For experiment tracking\n",
    "!pip install -q optuna  # For hyperparameter optimization\n",
    "!pip install -q redis  # For caching\n",
    "!pip install -q fastapi uvicorn  # For API\n",
    "\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "learning-engine"
   },
   "source": [
    "## 2. Enhanced Learning Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "learning-core"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "@dataclass\n",
    "class LearningPattern:\n",
    "    \"\"\"Represents a learned pattern from user interactions\"\"\"\n",
    "    pattern_id: str\n",
    "    pattern_type: str  # 'prompt_improvement', 'rejection_recovery', 'style_preference'\n",
    "    input_features: Dict[str, Any]\n",
    "    output_features: Dict[str, Any]\n",
    "    confidence: float\n",
    "    frequency: int\n",
    "    last_seen: datetime\n",
    "    success_rate: float\n",
    "    embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "@dataclass\n",
    "class UserLearningProfile:\n",
    "    \"\"\"Advanced user-specific learning profile\"\"\"\n",
    "    user_id: str\n",
    "    learned_patterns: List[LearningPattern] = field(default_factory=list)\n",
    "    preference_embeddings: Optional[np.ndarray] = None\n",
    "    skill_progression: Dict[str, float] = field(default_factory=dict)\n",
    "    interaction_history: List[Dict] = field(default_factory=list)\n",
    "    personalization_vector: Optional[np.ndarray] = None\n",
    "    \n",
    "class EnhancedLearningEngine:\n",
    "    \"\"\"GPU-accelerated learning engine for pattern recognition and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu: bool = True):\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda' if self.use_gpu else 'cpu')\n",
    "        print(f\"üöÄ Learning Engine initialized on {self.device}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        self._init_models()\n",
    "        \n",
    "        # Learning storage\n",
    "        self.user_profiles: Dict[str, UserLearningProfile] = {}\n",
    "        self.global_patterns: List[LearningPattern] = []\n",
    "        self.pattern_embeddings = None\n",
    "        \n",
    "    def _init_models(self):\n",
    "        \"\"\"Initialize ML models for learning\"\"\"\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        \n",
    "        # Sentence embedding model for semantic understanding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        if self.use_gpu:\n",
    "            self.embedding_model = self.embedding_model.to(self.device)\n",
    "        \n",
    "        # Pattern matching model\n",
    "        self.pattern_matcher = self._build_pattern_matcher()\n",
    "        \n",
    "        # Success prediction model\n",
    "        self.success_predictor = self._build_success_predictor()\n",
    "        \n",
    "    def _build_pattern_matcher(self):\n",
    "        \"\"\"Build neural network for pattern matching\"\"\"\n",
    "        import torch.nn as nn\n",
    "        \n",
    "        class PatternMatcher(nn.Module):\n",
    "            def __init__(self, input_dim=384, hidden_dim=256, output_dim=128):\n",
    "                super().__init__()\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, output_dim)\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                return self.encoder(x)\n",
    "        \n",
    "        model = PatternMatcher().to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def _build_success_predictor(self):\n",
    "        \"\"\"Build model to predict optimization success\"\"\"\n",
    "        import torch.nn as nn\n",
    "        \n",
    "        class SuccessPredictor(nn.Module):\n",
    "            def __init__(self, input_dim=512, hidden_dim=256):\n",
    "                super().__init__()\n",
    "                self.predictor = nn.Sequential(\n",
    "                    nn.Linear(input_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.3),\n",
    "                    nn.Linear(hidden_dim, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 1),\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "                \n",
    "            def forward(self, x):\n",
    "                return self.predictor(x)\n",
    "        \n",
    "        model = SuccessPredictor().to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def learn_from_interaction(self, user_id: str, interaction_data: Dict[str, Any]):\n",
    "        \"\"\"Learn from a user interaction\"\"\"\n",
    "        # Get or create user profile\n",
    "        if user_id not in self.user_profiles:\n",
    "            self.user_profiles[user_id] = UserLearningProfile(user_id=user_id)\n",
    "        \n",
    "        profile = self.user_profiles[user_id]\n",
    "        \n",
    "        # Extract features\n",
    "        input_embedding = self._embed_text(interaction_data.get('input', ''))\n",
    "        output_embedding = self._embed_text(interaction_data.get('output', ''))\n",
    "        \n",
    "        # Create learning pattern\n",
    "        pattern = LearningPattern(\n",
    "            pattern_id=f\"{user_id}_{datetime.now().timestamp()}\",\n",
    "            pattern_type=interaction_data.get('type', 'general'),\n",
    "            input_features=self._extract_features(interaction_data['input']),\n",
    "            output_features=self._extract_features(interaction_data['output']),\n",
    "            confidence=interaction_data.get('confidence', 0.8),\n",
    "            frequency=1,\n",
    "            last_seen=datetime.now(),\n",
    "            success_rate=interaction_data.get('success_rate', 0.85),\n",
    "            embeddings=np.concatenate([input_embedding, output_embedding])\n",
    "        )\n",
    "        \n",
    "        # Update profile\n",
    "        profile.learned_patterns.append(pattern)\n",
    "        profile.interaction_history.append(interaction_data)\n",
    "        \n",
    "        # Update skill progression\n",
    "        self._update_skill_progression(profile, interaction_data)\n",
    "        \n",
    "        # Update personalization vector\n",
    "        self._update_personalization_vector(profile)\n",
    "        \n",
    "        return pattern\n",
    "    \n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for text\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.embedding_model.encode(text, convert_to_tensor=True)\n",
    "            return embedding.cpu().numpy()\n",
    "    \n",
    "    def _extract_features(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract linguistic and semantic features\"\"\"\n",
    "        features = {\n",
    "            'length': len(text.split()),\n",
    "            'complexity': self._calculate_complexity(text),\n",
    "            'sentiment': self._analyze_sentiment(text),\n",
    "            'keywords': self._extract_keywords(text),\n",
    "            'style': self._detect_style(text)\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _calculate_complexity(self, text: str) -> float:\n",
    "        \"\"\"Calculate text complexity score\"\"\"\n",
    "        words = text.split()\n",
    "        avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "        unique_ratio = len(set(words)) / len(words) if words else 0\n",
    "        return (avg_word_length / 10 + unique_ratio) / 2\n",
    "    \n",
    "    def _analyze_sentiment(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Simple sentiment analysis\"\"\"\n",
    "        # Placeholder - in production use proper sentiment model\n",
    "        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful']\n",
    "        negative_words = ['bad', 'poor', 'terrible', 'awful', 'horrible']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        pos_score = sum(1 for word in positive_words if word in text_lower)\n",
    "        neg_score = sum(1 for word in negative_words if word in text_lower)\n",
    "        \n",
    "        total = pos_score + neg_score + 1\n",
    "        return {\n",
    "            'positive': pos_score / total,\n",
    "            'negative': neg_score / total,\n",
    "            'neutral': 1 - (pos_score + neg_score) / total\n",
    "        }\n",
    "    \n",
    "    def _extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key terms from text\"\"\"\n",
    "        # Simple keyword extraction\n",
    "        import re\n",
    "        words = re.findall(r'\\b\\w{4,}\\b', text.lower())\n",
    "        word_freq = defaultdict(int)\n",
    "        for word in words:\n",
    "            word_freq[word] += 1\n",
    "        \n",
    "        # Return top 5 most frequent words\n",
    "        return sorted(word_freq.keys(), key=word_freq.get, reverse=True)[:5]\n",
    "    \n",
    "    def _detect_style(self, text: str) -> str:\n",
    "        \"\"\"Detect writing style\"\"\"\n",
    "        if len(text.split()) < 10:\n",
    "            return 'concise'\n",
    "        elif any(word in text.lower() for word in ['cinematic', 'artistic', 'dramatic']):\n",
    "            return 'creative'\n",
    "        elif any(word in text.lower() for word in ['analyze', 'evaluate', 'examine']):\n",
    "            return 'analytical'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def _update_skill_progression(self, profile: UserLearningProfile, interaction: Dict):\n",
    "        \"\"\"Update user's skill progression\"\"\"\n",
    "        skill_type = interaction.get('skill_type', 'general')\n",
    "        success = interaction.get('success', True)\n",
    "        \n",
    "        if skill_type not in profile.skill_progression:\n",
    "            profile.skill_progression[skill_type] = 0.5\n",
    "        \n",
    "        # Update with exponential moving average\n",
    "        alpha = 0.1  # Learning rate\n",
    "        current = profile.skill_progression[skill_type]\n",
    "        profile.skill_progression[skill_type] = (1 - alpha) * current + alpha * (1.0 if success else 0.0)\n",
    "    \n",
    "    def _update_personalization_vector(self, profile: UserLearningProfile):\n",
    "        \"\"\"Update user's personalization vector\"\"\"\n",
    "        if len(profile.learned_patterns) < 3:\n",
    "            return\n",
    "        \n",
    "        # Aggregate pattern embeddings\n",
    "        recent_patterns = profile.learned_patterns[-20:]  # Last 20 patterns\n",
    "        embeddings = [p.embeddings for p in recent_patterns if p.embeddings is not None]\n",
    "        \n",
    "        if embeddings:\n",
    "            # Average embeddings weighted by success rate and recency\n",
    "            weights = []\n",
    "            for i, pattern in enumerate(recent_patterns):\n",
    "                recency_weight = (i + 1) / len(recent_patterns)\n",
    "                success_weight = pattern.success_rate\n",
    "                weights.append(recency_weight * success_weight)\n",
    "            \n",
    "            weights = np.array(weights) / np.sum(weights)\n",
    "            weighted_embeddings = np.average(embeddings, axis=0, weights=weights[:len(embeddings)])\n",
    "            profile.personalization_vector = weighted_embeddings\n",
    "\n",
    "# Initialize the learning engine\n",
    "learning_engine = EnhancedLearningEngine()\n",
    "print(\"‚úÖ Enhanced Learning Engine ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pattern-mining"
   },
   "source": [
    "## 3. Advanced Pattern Mining & Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pattern-miner"
   },
   "outputs": [],
   "source": [
    "class PatternMiner:\n",
    "    \"\"\"GPU-accelerated pattern mining from user interactions\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_engine: EnhancedLearningEngine):\n",
    "        self.engine = learning_engine\n",
    "        self.mined_patterns = []\n",
    "        \n",
    "    def mine_frequent_patterns(self, min_support: float = 0.1) -> List[Dict]:\n",
    "        \"\"\"Mine frequent patterns across all users\"\"\"\n",
    "        all_patterns = []\n",
    "        \n",
    "        # Collect all patterns\n",
    "        for profile in self.engine.user_profiles.values():\n",
    "            all_patterns.extend(profile.learned_patterns)\n",
    "        \n",
    "        if not all_patterns:\n",
    "            return []\n",
    "        \n",
    "        # Convert to embeddings matrix\n",
    "        embeddings = np.array([p.embeddings for p in all_patterns if p.embeddings is not None])\n",
    "        \n",
    "        if len(embeddings) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Use GPU for clustering\n",
    "        embeddings_tensor = torch.tensor(embeddings, device=self.engine.device)\n",
    "        \n",
    "        # Perform clustering to find pattern groups\n",
    "        from sklearn.cluster import DBSCAN\n",
    "        \n",
    "        # Move to CPU for sklearn\n",
    "        embeddings_cpu = embeddings_tensor.cpu().numpy()\n",
    "        \n",
    "        clustering = DBSCAN(eps=0.3, min_samples=max(2, int(len(embeddings) * min_support)))\n",
    "        labels = clustering.fit_predict(embeddings_cpu)\n",
    "        \n",
    "        # Extract pattern groups\n",
    "        pattern_groups = defaultdict(list)\n",
    "        for i, label in enumerate(labels):\n",
    "            if label != -1:  # Not noise\n",
    "                pattern_groups[label].append(all_patterns[i])\n",
    "        \n",
    "        # Analyze each group\n",
    "        mined_patterns = []\n",
    "        for group_id, patterns in pattern_groups.items():\n",
    "            if len(patterns) >= max(2, int(len(all_patterns) * min_support)):\n",
    "                pattern_info = self._analyze_pattern_group(patterns)\n",
    "                pattern_info['group_id'] = group_id\n",
    "                pattern_info['support'] = len(patterns) / len(all_patterns)\n",
    "                mined_patterns.append(pattern_info)\n",
    "        \n",
    "        self.mined_patterns = mined_patterns\n",
    "        return mined_patterns\n",
    "    \n",
    "    def _analyze_pattern_group(self, patterns: List[LearningPattern]) -> Dict:\n",
    "        \"\"\"Analyze a group of similar patterns\"\"\"\n",
    "        # Calculate group statistics\n",
    "        avg_confidence = np.mean([p.confidence for p in patterns])\n",
    "        avg_success = np.mean([p.success_rate for p in patterns])\n",
    "        \n",
    "        # Find common features\n",
    "        input_features = defaultdict(list)\n",
    "        output_features = defaultdict(list)\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for key, value in pattern.input_features.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    input_features[key].append(value)\n",
    "            for key, value in pattern.output_features.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    output_features[key].append(value)\n",
    "        \n",
    "        # Calculate feature statistics\n",
    "        input_stats = {k: {'mean': np.mean(v), 'std': np.std(v)} \n",
    "                      for k, v in input_features.items() if v}\n",
    "        output_stats = {k: {'mean': np.mean(v), 'std': np.std(v)} \n",
    "                       for k, v in output_features.items() if v}\n",
    "        \n",
    "        # Find most common pattern type\n",
    "        pattern_types = [p.pattern_type for p in patterns]\n",
    "        most_common_type = max(set(pattern_types), key=pattern_types.count)\n",
    "        \n",
    "        return {\n",
    "            'pattern_count': len(patterns),\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'avg_success_rate': avg_success,\n",
    "            'dominant_type': most_common_type,\n",
    "            'input_feature_stats': input_stats,\n",
    "            'output_feature_stats': output_stats,\n",
    "            'sample_patterns': patterns[:3]  # Keep some examples\n",
    "        }\n",
    "    \n",
    "    def find_optimization_rules(self) -> List[Dict]:\n",
    "        \"\"\"Extract optimization rules from mined patterns\"\"\"\n",
    "        rules = []\n",
    "        \n",
    "        for pattern_group in self.mined_patterns:\n",
    "            if pattern_group['avg_success_rate'] > 0.8:  # High success patterns\n",
    "                rule = {\n",
    "                    'rule_id': f\"rule_{pattern_group['group_id']}\",\n",
    "                    'condition': self._extract_condition(pattern_group),\n",
    "                    'action': self._extract_action(pattern_group),\n",
    "                    'confidence': pattern_group['avg_confidence'],\n",
    "                    'support': pattern_group['support'],\n",
    "                    'success_rate': pattern_group['avg_success_rate']\n",
    "                }\n",
    "                rules.append(rule)\n",
    "        \n",
    "        return sorted(rules, key=lambda r: r['confidence'] * r['support'], reverse=True)\n",
    "    \n",
    "    def _extract_condition(self, pattern_group: Dict) -> Dict:\n",
    "        \"\"\"Extract rule conditions from pattern group\"\"\"\n",
    "        conditions = {}\n",
    "        \n",
    "        # Input feature conditions\n",
    "        for feature, stats in pattern_group['input_feature_stats'].items():\n",
    "            if stats['std'] < stats['mean'] * 0.2:  # Low variance\n",
    "                conditions[f'input_{feature}'] = {\n",
    "                    'type': 'range',\n",
    "                    'min': stats['mean'] - stats['std'],\n",
    "                    'max': stats['mean'] + stats['std']\n",
    "                }\n",
    "        \n",
    "        conditions['pattern_type'] = pattern_group['dominant_type']\n",
    "        return conditions\n",
    "    \n",
    "    def _extract_action(self, pattern_group: Dict) -> Dict:\n",
    "        \"\"\"Extract rule actions from pattern group\"\"\"\n",
    "        actions = {}\n",
    "        \n",
    "        # Output feature actions\n",
    "        for feature, stats in pattern_group['output_feature_stats'].items():\n",
    "            actions[f'set_{feature}'] = stats['mean']\n",
    "        \n",
    "        # Add sample transformations\n",
    "        if pattern_group['sample_patterns']:\n",
    "            sample = pattern_group['sample_patterns'][0]\n",
    "            actions['sample_transformation'] = {\n",
    "                'from': sample.input_features,\n",
    "                'to': sample.output_features\n",
    "            }\n",
    "        \n",
    "        return actions\n",
    "\n",
    "# Create pattern miner\n",
    "pattern_miner = PatternMiner(learning_engine)\n",
    "print(\"‚úÖ Pattern Miner initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "realtime-learning"
   },
   "source": [
    "## 4. Real-time Learning & Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "realtime-adapter"
   },
   "outputs": [],
   "source": [
    "class RealtimeAdapter:\n",
    "    \"\"\"Real-time adaptation based on learned patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_engine: EnhancedLearningEngine, pattern_miner: PatternMiner):\n",
    "        self.engine = learning_engine\n",
    "        self.miner = pattern_miner\n",
    "        self.adaptation_cache = {}\n",
    "        self.performance_history = []\n",
    "        \n",
    "    def adapt_optimization(self, user_id: str, prompt: str, context: Dict[str, Any]) -> Dict:\n",
    "        \"\"\"Adapt optimization strategy based on learned patterns\"\"\"\n",
    "        # Get user profile\n",
    "        profile = self.engine.user_profiles.get(user_id)\n",
    "        \n",
    "        # Generate prompt embedding\n",
    "        prompt_embedding = self.engine._embed_text(prompt)\n",
    "        \n",
    "        # Find similar successful patterns\n",
    "        similar_patterns = self._find_similar_patterns(prompt_embedding, profile)\n",
    "        \n",
    "        # Apply learned optimizations\n",
    "        optimization_strategy = self._build_optimization_strategy(similar_patterns, context)\n",
    "        \n",
    "        # Predict success probability\n",
    "        success_probability = self._predict_success(prompt_embedding, optimization_strategy)\n",
    "        \n",
    "        # Cache the adaptation\n",
    "        cache_key = f\"{user_id}_{hash(prompt)}\"\n",
    "        self.adaptation_cache[cache_key] = {\n",
    "            'strategy': optimization_strategy,\n",
    "            'probability': success_probability,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'adapted_strategy': optimization_strategy,\n",
    "            'success_probability': float(success_probability),\n",
    "            'similar_patterns_found': len(similar_patterns),\n",
    "            'personalization_applied': profile is not None,\n",
    "            'confidence': self._calculate_confidence(similar_patterns)\n",
    "        }\n",
    "    \n",
    "    def _find_similar_patterns(self, embedding: np.ndarray, profile: Optional[UserLearningProfile]) -> List[LearningPattern]:\n",
    "        \"\"\"Find patterns similar to current input\"\"\"\n",
    "        similar_patterns = []\n",
    "        \n",
    "        # Search in user's patterns first\n",
    "        if profile and profile.learned_patterns:\n",
    "            user_patterns = [(p, self._calculate_similarity(embedding, p.embeddings)) \n",
    "                           for p in profile.learned_patterns if p.embeddings is not None]\n",
    "            user_patterns.sort(key=lambda x: x[1], reverse=True)\n",
    "            similar_patterns.extend([p for p, sim in user_patterns[:5] if sim > 0.7])\n",
    "        \n",
    "        # Search in global patterns\n",
    "        if self.miner.mined_patterns:\n",
    "            for pattern_group in self.miner.mined_patterns:\n",
    "                for sample_pattern in pattern_group['sample_patterns']:\n",
    "                    if sample_pattern.embeddings is not None:\n",
    "                        sim = self._calculate_similarity(embedding, sample_pattern.embeddings[:len(embedding)])\n",
    "                        if sim > 0.75:\n",
    "                            similar_patterns.append(sample_pattern)\n",
    "        \n",
    "        return similar_patterns[:10]  # Top 10 most similar\n",
    "    \n",
    "    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between embeddings\"\"\"\n",
    "        if embedding2 is None or len(embedding1) != len(embedding2):\n",
    "            return 0.0\n",
    "        \n",
    "        dot_product = np.dot(embedding1, embedding2)\n",
    "        norm1 = np.linalg.norm(embedding1)\n",
    "        norm2 = np.linalg.norm(embedding2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def _build_optimization_strategy(self, patterns: List[LearningPattern], context: Dict) -> Dict:\n",
    "        \"\"\"Build optimization strategy from similar patterns\"\"\"\n",
    "        if not patterns:\n",
    "            return self._default_strategy()\n",
    "        \n",
    "        # Aggregate strategies from successful patterns\n",
    "        strategy = {\n",
    "            'approach': 'adaptive',\n",
    "            'techniques': [],\n",
    "            'parameters': {},\n",
    "            'focus_areas': []\n",
    "        }\n",
    "        \n",
    "        # Analyze pattern types\n",
    "        pattern_types = [p.pattern_type for p in patterns]\n",
    "        type_counts = defaultdict(int)\n",
    "        for ptype in pattern_types:\n",
    "            type_counts[ptype] += 1\n",
    "        \n",
    "        # Set primary approach based on most common pattern type\n",
    "        primary_type = max(type_counts.keys(), key=type_counts.get)\n",
    "        strategy['approach'] = self._map_pattern_type_to_approach(primary_type)\n",
    "        \n",
    "        # Extract techniques from successful patterns\n",
    "        for pattern in patterns:\n",
    "            if pattern.success_rate > 0.8:\n",
    "                technique = self._extract_technique(pattern)\n",
    "                if technique and technique not in strategy['techniques']:\n",
    "                    strategy['techniques'].append(technique)\n",
    "        \n",
    "        # Set parameters based on pattern features\n",
    "        strategy['parameters'] = self._aggregate_parameters(patterns)\n",
    "        \n",
    "        # Identify focus areas\n",
    "        strategy['focus_areas'] = self._identify_focus_areas(patterns)\n",
    "        \n",
    "        return strategy\n",
    "    \n",
    "    def _default_strategy(self) -> Dict:\n",
    "        \"\"\"Default optimization strategy\"\"\"\n",
    "        return {\n",
    "            'approach': 'balanced',\n",
    "            'techniques': ['enhance_clarity', 'add_context', 'improve_specificity'],\n",
    "            'parameters': {\n",
    "                'enhancement_level': 0.7,\n",
    "                'context_depth': 'moderate',\n",
    "                'safety_check': True\n",
    "            },\n",
    "            'focus_areas': ['clarity', 'relevance', 'safety']\n",
    "        }\n",
    "    \n",
    "    def _map_pattern_type_to_approach(self, pattern_type: str) -> str:\n",
    "        \"\"\"Map pattern type to optimization approach\"\"\"\n",
    "        mapping = {\n",
    "            'prompt_improvement': 'enhancement',\n",
    "            'rejection_recovery': 'safety_first',\n",
    "            'style_preference': 'style_adaptive',\n",
    "            'context_expansion': 'context_rich',\n",
    "            'creative_enhancement': 'creative'\n",
    "        }\n",
    "        return mapping.get(pattern_type, 'balanced')\n",
    "    \n",
    "    def _extract_technique(self, pattern: LearningPattern) -> Optional[str]:\n",
    "        \"\"\"Extract optimization technique from pattern\"\"\"\n",
    "        # Analyze input/output differences\n",
    "        input_complexity = pattern.input_features.get('complexity', 0)\n",
    "        output_complexity = pattern.output_features.get('complexity', 0)\n",
    "        \n",
    "        if output_complexity > input_complexity * 1.2:\n",
    "            return 'expand_details'\n",
    "        elif pattern.output_features.get('style') == 'creative':\n",
    "            return 'add_creativity'\n",
    "        elif len(pattern.output_features.get('keywords', [])) > len(pattern.input_features.get('keywords', [])):\n",
    "            return 'enrich_vocabulary'\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _aggregate_parameters(self, patterns: List[LearningPattern]) -> Dict:\n",
    "        \"\"\"Aggregate optimization parameters from patterns\"\"\"\n",
    "        params = {\n",
    "            'enhancement_level': 0.7,\n",
    "            'creativity_boost': 0.0,\n",
    "            'safety_threshold': 0.8,\n",
    "            'context_depth': 'moderate'\n",
    "        }\n",
    "        \n",
    "        # Calculate average success-weighted parameters\n",
    "        if patterns:\n",
    "            avg_complexity = np.mean([p.output_features.get('complexity', 0.5) for p in patterns])\n",
    "            params['enhancement_level'] = min(0.9, avg_complexity * 1.2)\n",
    "            \n",
    "            creative_patterns = [p for p in patterns if p.output_features.get('style') == 'creative']\n",
    "            if len(creative_patterns) > len(patterns) * 0.3:\n",
    "                params['creativity_boost'] = 0.3\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def _identify_focus_areas(self, patterns: List[LearningPattern]) -> List[str]:\n",
    "        \"\"\"Identify areas to focus on based on patterns\"\"\"\n",
    "        focus_areas = set()\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            # Check for consistent improvements\n",
    "            if pattern.output_features.get('clarity', 0) > pattern.input_features.get('clarity', 0):\n",
    "                focus_areas.add('clarity')\n",
    "            if pattern.pattern_type == 'style_preference':\n",
    "                focus_areas.add('style')\n",
    "            if pattern.pattern_type == 'rejection_recovery':\n",
    "                focus_areas.add('safety')\n",
    "        \n",
    "        return list(focus_areas) or ['general']\n",
    "    \n",
    "    def _predict_success(self, embedding: np.ndarray, strategy: Dict) -> float:\n",
    "        \"\"\"Predict success probability using neural network\"\"\"\n",
    "        # Prepare input features\n",
    "        strategy_features = self._encode_strategy(strategy)\n",
    "        \n",
    "        # Combine embeddings and strategy\n",
    "        combined_features = np.concatenate([embedding, strategy_features])\n",
    "        \n",
    "        # Ensure correct input dimension\n",
    "        if len(combined_features) < 512:\n",
    "            combined_features = np.pad(combined_features, (0, 512 - len(combined_features)))\n",
    "        elif len(combined_features) > 512:\n",
    "            combined_features = combined_features[:512]\n",
    "        \n",
    "        # Convert to tensor\n",
    "        input_tensor = torch.tensor(combined_features, dtype=torch.float32).unsqueeze(0).to(self.engine.device)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            self.engine.success_predictor.eval()\n",
    "            prediction = self.engine.success_predictor(input_tensor)\n",
    "            return prediction.item()\n",
    "    \n",
    "    def _encode_strategy(self, strategy: Dict) -> np.ndarray:\n",
    "        \"\"\"Encode strategy as feature vector\"\"\"\n",
    "        # Simple encoding - in production use more sophisticated encoding\n",
    "        features = []\n",
    "        \n",
    "        # Approach encoding (one-hot)\n",
    "        approaches = ['adaptive', 'enhancement', 'safety_first', 'creative', 'balanced']\n",
    "        approach_vec = [1.0 if strategy['approach'] == a else 0.0 for a in approaches]\n",
    "        features.extend(approach_vec)\n",
    "        \n",
    "        # Technique count\n",
    "        features.append(len(strategy['techniques']) / 10.0)\n",
    "        \n",
    "        # Parameter encoding\n",
    "        features.append(strategy['parameters'].get('enhancement_level', 0.5))\n",
    "        features.append(strategy['parameters'].get('creativity_boost', 0.0))\n",
    "        features.append(strategy['parameters'].get('safety_threshold', 0.8))\n",
    "        \n",
    "        # Focus area count\n",
    "        features.append(len(strategy['focus_areas']) / 5.0)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _calculate_confidence(self, patterns: List[LearningPattern]) -> float:\n",
    "        \"\"\"Calculate confidence score based on pattern evidence\"\"\"\n",
    "        if not patterns:\n",
    "            return 0.5\n",
    "        \n",
    "        # Factors: number of patterns, average success rate, recency\n",
    "        pattern_count_score = min(1.0, len(patterns) / 10.0)\n",
    "        avg_success = np.mean([p.success_rate for p in patterns])\n",
    "        \n",
    "        # Recency score\n",
    "        now = datetime.now()\n",
    "        recency_scores = []\n",
    "        for p in patterns:\n",
    "            days_old = (now - p.last_seen).days\n",
    "            recency_scores.append(max(0, 1 - days_old / 30))  # Decay over 30 days\n",
    "        avg_recency = np.mean(recency_scores)\n",
    "        \n",
    "        # Weighted combination\n",
    "        confidence = 0.3 * pattern_count_score + 0.5 * avg_success + 0.2 * avg_recency\n",
    "        return float(confidence)\n",
    "\n",
    "# Create realtime adapter\n",
    "realtime_adapter = RealtimeAdapter(learning_engine, pattern_miner)\n",
    "print(\"‚úÖ Realtime Adapter ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api-integration"
   },
   "source": [
    "## 5. API Integration with Your Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "api-server"
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow nested asyncio for Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# API Models\n",
    "class LearningInteraction(BaseModel):\n",
    "    user_id: str\n",
    "    input_prompt: str\n",
    "    output_prompt: str\n",
    "    interaction_type: str = 'general'\n",
    "    success: bool = True\n",
    "    confidence: float = 0.8\n",
    "    metadata: Optional[Dict] = None\n",
    "\n",
    "class OptimizationRequest(BaseModel):\n",
    "    user_id: str\n",
    "    prompt: str\n",
    "    context: Optional[Dict] = None\n",
    "\n",
    "class PatternMiningRequest(BaseModel):\n",
    "    min_support: float = 0.1\n",
    "    user_filter: Optional[str] = None\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"AI Platform Learning Engine\", version=\"1.0.0\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\n",
    "        \"message\": \"AI Platform Learning Engine API\",\n",
    "        \"endpoints\": [\n",
    "            \"/learn\",\n",
    "            \"/adapt\",\n",
    "            \"/mine-patterns\",\n",
    "            \"/user-profile/{user_id}\",\n",
    "            \"/learning-stats\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.post(\"/learn\")\n",
    "async def learn_from_interaction(interaction: LearningInteraction):\n",
    "    \"\"\"Record and learn from a user interaction\"\"\"\n",
    "    try:\n",
    "        # Prepare interaction data\n",
    "        interaction_data = {\n",
    "            'input': interaction.input_prompt,\n",
    "            'output': interaction.output_prompt,\n",
    "            'type': interaction.interaction_type,\n",
    "            'success': interaction.success,\n",
    "            'confidence': interaction.confidence,\n",
    "            'success_rate': 1.0 if interaction.success else 0.0\n",
    "        }\n",
    "        \n",
    "        if interaction.metadata:\n",
    "            interaction_data.update(interaction.metadata)\n",
    "        \n",
    "        # Learn from interaction\n",
    "        pattern = learning_engine.learn_from_interaction(\n",
    "            interaction.user_id,\n",
    "            interaction_data\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"pattern_id\": pattern.pattern_id,\n",
    "            \"message\": \"Interaction learned successfully\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/adapt\")\n",
    "async def adapt_optimization(request: OptimizationRequest):\n",
    "    \"\"\"Get adapted optimization strategy based on learning\"\"\"\n",
    "    try:\n",
    "        result = realtime_adapter.adapt_optimization(\n",
    "            request.user_id,\n",
    "            request.prompt,\n",
    "            request.context or {}\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"adaptation\": result\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/mine-patterns\")\n",
    "async def mine_patterns(request: PatternMiningRequest):\n",
    "    \"\"\"Mine patterns from all interactions\"\"\"\n",
    "    try:\n",
    "        patterns = pattern_miner.mine_frequent_patterns(request.min_support)\n",
    "        rules = pattern_miner.find_optimization_rules()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"patterns_found\": len(patterns),\n",
    "            \"rules_extracted\": len(rules),\n",
    "            \"patterns\": patterns[:10],  # Top 10\n",
    "            \"rules\": rules[:5]  # Top 5 rules\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/user-profile/{user_id}\")\n",
    "async def get_user_profile(user_id: str):\n",
    "    \"\"\"Get user learning profile\"\"\"\n",
    "    profile = learning_engine.user_profiles.get(user_id)\n",
    "    \n",
    "    if not profile:\n",
    "        raise HTTPException(status_code=404, detail=\"User profile not found\")\n",
    "    \n",
    "    return {\n",
    "        \"user_id\": profile.user_id,\n",
    "        \"pattern_count\": len(profile.learned_patterns),\n",
    "        \"skill_progression\": profile.skill_progression,\n",
    "        \"interaction_count\": len(profile.interaction_history),\n",
    "        \"has_personalization\": profile.personalization_vector is not None\n",
    "    }\n",
    "\n",
    "@app.get(\"/learning-stats\")\n",
    "async def get_learning_stats():\n",
    "    \"\"\"Get overall learning statistics\"\"\"\n",
    "    total_users = len(learning_engine.user_profiles)\n",
    "    total_patterns = sum(len(p.learned_patterns) for p in learning_engine.user_profiles.values())\n",
    "    \n",
    "    return {\n",
    "        \"total_users\": total_users,\n",
    "        \"total_patterns\": total_patterns,\n",
    "        \"mined_pattern_groups\": len(pattern_miner.mined_patterns),\n",
    "        \"gpu_enabled\": learning_engine.use_gpu,\n",
    "        \"device\": str(learning_engine.device)\n",
    "    }\n",
    "\n",
    "# Function to run the server\n",
    "def run_api_server(port=8000):\n",
    "    \"\"\"Run the API server\"\"\"\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    server.run()\n",
    "\n",
    "print(\"‚úÖ API server defined. Run run_api_server() to start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo-section"
   },
   "source": [
    "## 6. Demo: Testing the Learning Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo-test"
   },
   "outputs": [],
   "source": [
    "# Demo: Simulate learning from interactions\n",
    "print(\"üéØ Demo: Testing Learning Engine\\n\")\n",
    "\n",
    "# Simulate some user interactions\n",
    "demo_interactions = [\n",
    "    {\n",
    "        \"user_id\": \"user123\",\n",
    "        \"input\": \"elderly man and young woman in bedroom\",\n",
    "        \"output\": \"An intergenerational mentorship scene in a well-lit study, featuring a distinguished elderly professor and his young prot√©g√© engaged in academic discussion\",\n",
    "        \"type\": \"rejection_recovery\",\n",
    "        \"success\": True,\n",
    "        \"confidence\": 0.92\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user123\",\n",
    "        \"input\": \"romantic scene between couple\",\n",
    "        \"output\": \"A cinematic portrayal of deep emotional connection between two souls, captured in golden hour lighting with artistic composition emphasizing their profound bond\",\n",
    "        \"type\": \"style_preference\",\n",
    "        \"success\": True,\n",
    "        \"confidence\": 0.88\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"user456\",\n",
    "        \"input\": \"futuristic city at night\",\n",
    "        \"output\": \"A breathtaking cyberpunk metropolis illuminated by neon lights, with flying vehicles weaving between towering skyscrapers under a starlit sky, rendered in cinematic detail\",\n",
    "        \"type\": \"creative_enhancement\",\n",
    "        \"success\": True,\n",
    "        \"confidence\": 0.95\n",
    "    }\n",
    "]\n",
    "\n",
    "# Learn from interactions\n",
    "for interaction in demo_interactions:\n",
    "    pattern = learning_engine.learn_from_interaction(\n",
    "        interaction[\"user_id\"],\n",
    "        interaction\n",
    "    )\n",
    "    print(f\"‚úÖ Learned pattern: {pattern.pattern_type} (confidence: {pattern.confidence:.2f})\")\n",
    "\n",
    "# Mine patterns\n",
    "print(\"\\nüîç Mining patterns...\")\n",
    "mined_patterns = pattern_miner.mine_frequent_patterns(min_support=0.1)\n",
    "print(f\"Found {len(mined_patterns)} pattern groups\")\n",
    "\n",
    "# Extract rules\n",
    "rules = pattern_miner.find_optimization_rules()\n",
    "print(f\"Extracted {len(rules)} optimization rules\")\n",
    "\n",
    "# Test adaptation\n",
    "print(\"\\nüéØ Testing real-time adaptation...\")\n",
    "test_prompt = \"old man and young girl talking\"\n",
    "adaptation = realtime_adapter.adapt_optimization(\"user123\", test_prompt, {})\n",
    "\n",
    "print(f\"\\nAdaptation Result:\")\n",
    "print(f\"  Strategy: {adaptation['adapted_strategy']['approach']}\")\n",
    "print(f\"  Success Probability: {adaptation['success_probability']:.2%}\")\n",
    "print(f\"  Confidence: {adaptation['confidence']:.2%}\")\n",
    "print(f\"  Techniques: {', '.join(adaptation['adapted_strategy']['techniques'][:3])}\")\n",
    "\n",
    "# Show user profile\n",
    "print(\"\\nüë§ User Profile Summary:\")\n",
    "profile = learning_engine.user_profiles.get(\"user123\")\n",
    "if profile:\n",
    "    print(f\"  Patterns Learned: {len(profile.learned_patterns)}\")\n",
    "    print(f\"  Skills: {list(profile.skill_progression.keys())}\")\n",
    "    print(f\"  Has Personalization: {profile.personalization_vector is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "integration-section"
   },
   "source": [
    "## 7. Integration with Your Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "integration-code"
   },
   "outputs": [],
   "source": [
    "# Code to integrate with your Flask app\n",
    "print(\"üìã Integration Code for Your Flask App:\\n\")\n",
    "\n",
    "integration_code = '''\n",
    "# Add to your Flask app.py:\n",
    "\n",
    "import requests\n",
    "from functools import wraps\n",
    "\n",
    "# Configuration\n",
    "COLAB_LEARNING_API = \"https://YOUR_NGROK_URL\"  # Replace with your ngrok URL\n",
    "\n",
    "def with_learning(f):\n",
    "    \"\"\"Decorator to add learning capability to endpoints\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        # Execute original function\n",
    "        result = f(*args, **kwargs)\n",
    "        \n",
    "        # Extract learning data if successful\n",
    "        if result.get_json() and result.get_json().get('success'):\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                response = result.get_json()\n",
    "                \n",
    "                # Send to learning engine\n",
    "                learning_data = {\n",
    "                    \"user_id\": g.get('user_id', 'anonymous'),\n",
    "                    \"input_prompt\": data.get('prompt', ''),\n",
    "                    \"output_prompt\": response.get('optimized', ''),\n",
    "                    \"interaction_type\": data.get('type', 'optimization'),\n",
    "                    \"success\": True,\n",
    "                    \"confidence\": response.get('confidence', 0.8)\n",
    "                }\n",
    "                \n",
    "                # Async call to learning API\n",
    "                requests.post(f\"{COLAB_LEARNING_API}/learn\", json=learning_data, timeout=1)\n",
    "            except:\n",
    "                pass  # Don't break main flow if learning fails\n",
    "        \n",
    "        return result\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/api/optimize-with-learning', methods=['POST'])\n",
    "@secure_endpoint\n",
    "@with_learning\n",
    "def optimize_with_learning():\n",
    "    \"\"\"Optimization endpoint with learning capability\"\"\"\n",
    "    data = request.get_json()\n",
    "    prompt = data.get('prompt', '')\n",
    "    user_id = g.get('user_id', 'anonymous')\n",
    "    \n",
    "    # Get adapted strategy from learning engine\n",
    "    try:\n",
    "        adaptation_response = requests.post(\n",
    "            f\"{COLAB_LEARNING_API}/adapt\",\n",
    "            json={\n",
    "                \"user_id\": user_id,\n",
    "                \"prompt\": prompt,\n",
    "                \"context\": {\"source\": \"web_app\"}\n",
    "            },\n",
    "            timeout=2\n",
    "        )\n",
    "        \n",
    "        if adaptation_response.status_code == 200:\n",
    "            adaptation = adaptation_response.json()['adaptation']\n",
    "            # Use adapted strategy for optimization\n",
    "            # ... your optimization logic here ...\n",
    "    except:\n",
    "        # Fallback to default optimization\n",
    "        pass\n",
    "    \n",
    "    # Continue with normal optimization\n",
    "    return unified_optimize()\n",
    "'''\n",
    "\n",
    "print(integration_code)\n",
    "\n",
    "# Save integration code\n",
    "with open('/content/flask_integration.py', 'w') as f:\n",
    "    f.write(integration_code)\n",
    "print(\"\\n‚úÖ Integration code saved to /content/flask_integration.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deployment-section"
   },
   "source": [
    "## 8. Deploy Learning API with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy-ngrok"
   },
   "outputs": [],
   "source": [
    "# Install and setup ngrok for external access\n",
    "!pip install -q pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# Start API server in background\n",
    "def start_server():\n",
    "    run_api_server(port=8000)\n",
    "\n",
    "# Run server in thread\n",
    "server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Give server time to start\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"\\nüåê Learning Engine API available at: {public_url}\")\n",
    "print(f\"\\nüìã Use this URL in your Flask app: {public_url}\")\n",
    "print(\"\\n‚úÖ Learning Engine is running and accessible from anywhere!\")\n",
    "\n",
    "# Test the API\n",
    "import requests\n",
    "test_response = requests.get(f\"{public_url}/\")\n",
    "print(f\"\\nüß™ API Test: {test_response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitoring-section"
   },
   "source": [
    "## 9. Learning Analytics & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analytics-viz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create learning analytics dashboard\n",
    "def plot_learning_analytics():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('AI Platform Learning Analytics', fontsize=16)\n",
    "    \n",
    "    # 1. Pattern Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    pattern_types = defaultdict(int)\n",
    "    for profile in learning_engine.user_profiles.values():\n",
    "        for pattern in profile.learned_patterns:\n",
    "            pattern_types[pattern.pattern_type] += 1\n",
    "    \n",
    "    if pattern_types:\n",
    "        pd.Series(pattern_types).plot(kind='bar', ax=ax1, color='skyblue')\n",
    "        ax1.set_title('Pattern Type Distribution')\n",
    "        ax1.set_xlabel('Pattern Type')\n",
    "        ax1.set_ylabel('Count')\n",
    "    \n",
    "    # 2. Success Rate Over Time\n",
    "    ax2 = axes[0, 1]\n",
    "    success_rates = []\n",
    "    for profile in learning_engine.user_profiles.values():\n",
    "        for pattern in profile.learned_patterns:\n",
    "            success_rates.append({\n",
    "                'time': pattern.last_seen,\n",
    "                'success_rate': pattern.success_rate\n",
    "            })\n",
    "    \n",
    "    if success_rates:\n",
    "        df_success = pd.DataFrame(success_rates)\n",
    "        df_success.set_index('time')['success_rate'].plot(ax=ax2, marker='o')\n",
    "        ax2.set_title('Success Rate Trend')\n",
    "        ax2.set_ylabel('Success Rate')\n",
    "    \n",
    "    # 3. User Skill Progression\n",
    "    ax3 = axes[1, 0]\n",
    "    skill_data = []\n",
    "    for user_id, profile in learning_engine.user_profiles.items():\n",
    "        for skill, level in profile.skill_progression.items():\n",
    "            skill_data.append({\n",
    "                'user': user_id,\n",
    "                'skill': skill,\n",
    "                'level': level\n",
    "            })\n",
    "    \n",
    "    if skill_data:\n",
    "        df_skills = pd.DataFrame(skill_data)\n",
    "        pivot_skills = df_skills.pivot(index='user', columns='skill', values='level')\n",
    "        sns.heatmap(pivot_skills, ax=ax3, cmap='YlOrRd', annot=True, fmt='.2f')\n",
    "        ax3.set_title('User Skill Levels')\n",
    "    \n",
    "    # 4. Pattern Confidence Distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    confidences = []\n",
    "    for profile in learning_engine.user_profiles.values():\n",
    "        confidences.extend([p.confidence for p in profile.learned_patterns])\n",
    "    \n",
    "    if confidences:\n",
    "        ax4.hist(confidences, bins=20, color='lightgreen', edgecolor='black')\n",
    "        ax4.set_title('Pattern Confidence Distribution')\n",
    "        ax4.set_xlabel('Confidence')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate analytics\n",
    "plot_learning_analytics()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìä Learning Engine Statistics:\")\n",
    "print(f\"Total Users: {len(learning_engine.user_profiles)}\")\n",
    "print(f\"Total Patterns Learned: {sum(len(p.learned_patterns) for p in learning_engine.user_profiles.values())}\")\n",
    "print(f\"Pattern Groups Mined: {len(pattern_miner.mined_patterns)}\")\n",
    "print(f\"GPU Acceleration: {'Enabled' if learning_engine.use_gpu else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-section"
   },
   "source": [
    "## 10. Save & Export Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-models"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create export directory\n",
    "export_dir = \"/content/learning_engine_export\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Save learning engine state\n",
    "print(\"üíæ Saving learning engine state...\")\n",
    "\n",
    "# 1. Save user profiles\n",
    "profiles_data = {}\n",
    "for user_id, profile in learning_engine.user_profiles.items():\n",
    "    profiles_data[user_id] = {\n",
    "        'pattern_count': len(profile.learned_patterns),\n",
    "        'skill_progression': profile.skill_progression,\n",
    "        'interaction_count': len(profile.interaction_history)\n",
    "    }\n",
    "\n",
    "with open(f\"{export_dir}/user_profiles.json\", 'w') as f:\n",
    "    json.dump(profiles_data, f, indent=2)\n",
    "print(\"‚úÖ User profiles saved\")\n",
    "\n",
    "# 2. Save mined patterns\n",
    "with open(f\"{export_dir}/mined_patterns.pkl\", 'wb') as f:\n",
    "    pickle.dump(pattern_miner.mined_patterns, f)\n",
    "print(\"‚úÖ Mined patterns saved\")\n",
    "\n",
    "# 3. Save model weights\n",
    "torch.save({\n",
    "    'pattern_matcher': learning_engine.pattern_matcher.state_dict(),\n",
    "    'success_predictor': learning_engine.success_predictor.state_dict(),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}, f\"{export_dir}/model_weights.pt\")\n",
    "print(\"‚úÖ Model weights saved\")\n",
    "\n",
    "# 4. Create deployment package\n",
    "deployment_code = '''\n",
    "# Deployment code for your server\n",
    "# This loads the trained models and provides learning capability\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "class LearningSingleton:\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "            cls._instance.initialized = False\n",
    "        return cls._instance\n",
    "    \n",
    "    def initialize(self, model_path):\n",
    "        if not self.initialized:\n",
    "            # Load models and data\n",
    "            self.load_models(model_path)\n",
    "            self.initialized = True\n",
    "    \n",
    "    def load_models(self, path):\n",
    "        # Implementation to load saved models\n",
    "        pass\n",
    "\n",
    "# Usage in Flask:\n",
    "learning = LearningSingleton()\n",
    "learning.initialize('/path/to/models')\n",
    "'''\n",
    "\n",
    "with open(f\"{export_dir}/deployment.py\", 'w') as f:\n",
    "    f.write(deployment_code)\n",
    "\n",
    "# Create zip archive\n",
    "!cd /content && zip -r learning_engine_export.zip learning_engine_export/\n",
    "\n",
    "print(\"\\n‚úÖ Learning engine exported successfully!\")\n",
    "print(f\"üì¶ Download: /content/learning_engine_export.zip\")\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"1. Download the export package\")\n",
    "print(\"2. Integrate with your Flask app using the provided code\")\n",
    "print(\"3. The learning engine will continuously improve your platform!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}