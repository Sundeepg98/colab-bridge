{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üåü Sun Colab AI Processing Server\n",
    "\n",
    "**Notebook URL**: https://colab.research.google.com/drive/1EwfBj0nC9St-2hB1bv2zGWWDTcS4slsx\n",
    "\n",
    "**Secret Key**: `sun_colab`\n",
    "\n",
    "This notebook provides GPU-accelerated AI processing for your platform with enhanced security using Colab secrets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "security-setup"
   },
   "source": [
    "## üîê Security Setup\n",
    "\n",
    "First, let's set up secure access using Colab secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-secrets"
   },
   "outputs": [],
   "source": [
    "# Setup Colab secrets and authentication\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"üîê Setting up secure authentication...\")\n",
    "\n",
    "# Get the sun_colab secret key\n",
    "try:\n",
    "    sun_colab_key = userdata.get('sun_colab')\n",
    "    print(\"‚úÖ Successfully retrieved sun_colab secret\")\n",
    "    \n",
    "    # Set as environment variable for the session\n",
    "    os.environ['SUN_COLAB_KEY'] = sun_colab_key\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to get sun_colab secret: {e}\")\n",
    "    print(\"\\nüìù To set up the secret:\")\n",
    "    print(\"1. Click the üîë key icon in the left sidebar\")\n",
    "    print(\"2. Add a new secret named 'sun_colab'\")\n",
    "    print(\"3. Set a secure API key value\")\n",
    "    print(\"4. Re-run this cell\")\n",
    "    \n",
    "    # Generate a temporary key for demonstration\n",
    "    import secrets\n",
    "    temp_key = f\"sun_colab_{secrets.token_hex(16)}\"\n",
    "    os.environ['SUN_COLAB_KEY'] = temp_key\n",
    "    print(f\"\\n‚ö†Ô∏è  Using temporary key: {temp_key[:20]}...\")\n",
    "\n",
    "# Verify GPU access\n",
    "import torch\n",
    "print(f\"\\nüöÄ GPU Status: {'‚úÖ Available' if torch.cuda.is_available() else '‚ùå Not Available'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-deps"
   },
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages with optimized versions\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers[torch] accelerate\n",
    "!pip install -q diffusers==0.21.4\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q fastapi uvicorn[standard]\n",
    "!pip install -q pyngrok\n",
    "!pip install -q xformers\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q pillow==9.5.0\n",
    "!pip install -q requests aiohttp\n",
    "!pip install -q psutil GPUtil\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "processing-engine"
   },
   "source": [
    "## üß† Sun Colab Processing Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main-processor"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import asyncio\n",
    "import time\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SunColabProcessor:\n",
    "    \"\"\"Main processing engine for Sun Colab integration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.api_key = os.environ.get('SUN_COLAB_KEY')\n",
    "        self.session_id = hashlib.md5(f\"sun_colab_{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "        \n",
    "        logger.info(f\"üåü Sun Colab Processor initialized\")\n",
    "        logger.info(f\"   Device: {self.device}\")\n",
    "        logger.info(f\"   Session ID: {self.session_id}\")\n",
    "        logger.info(f\"   API Key: {'‚úÖ Set' if self.api_key else '‚ùå Missing'}\")\n",
    "        \n",
    "        # Model cache\n",
    "        self.models = {}\n",
    "        self.stats = {\n",
    "            'session_start': datetime.now(),\n",
    "            'tasks_processed': 0,\n",
    "            'total_processing_time': 0,\n",
    "            'models_loaded': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "        \n",
    "        # Initialize core models\n",
    "        self.text_model = None\n",
    "        self.image_model = None\n",
    "        self.embedding_model = None\n",
    "        \n",
    "    def authenticate_request(self, request_key: str) -> bool:\n",
    "        \"\"\"Authenticate incoming requests\"\"\"\n",
    "        return request_key == self.api_key\n",
    "    \n",
    "    async def load_text_model(self, model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "        \"\"\"Load text generation model with optimization\"\"\"\n",
    "        if self.text_model is None:\n",
    "            logger.info(f\"üî§ Loading text model: {model_name}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            \n",
    "            try:\n",
    "                self.text_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.text_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                self.text_model.to(self.device)\n",
    "                self.text_model.eval()\n",
    "                \n",
    "                # Add padding token\n",
    "                if self.text_tokenizer.pad_token is None:\n",
    "                    self.text_tokenizer.pad_token = self.text_tokenizer.eos_token\n",
    "                \n",
    "                load_time = time.time() - start_time\n",
    "                self.stats['models_loaded'] += 1\n",
    "                logger.info(f\"‚úÖ Text model loaded in {load_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to load text model: {e}\")\n",
    "                self.stats['errors'] += 1\n",
    "                raise\n",
    "    \n",
    "    async def load_image_model(self, model_id: str = \"runwayml/stable-diffusion-v1-5\"):\n",
    "        \"\"\"Load image generation model with memory optimization\"\"\"\n",
    "        if self.image_model is None:\n",
    "            logger.info(f\"üé® Loading image model: {model_id}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
    "            \n",
    "            try:\n",
    "                self.image_model = StableDiffusionPipeline.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,\n",
    "                    safety_checker=None,\n",
    "                    requires_safety_checker=False\n",
    "                )\n",
    "                \n",
    "                # Optimize scheduler\n",
    "                self.image_model.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                    self.image_model.scheduler.config\n",
    "                )\n",
    "                \n",
    "                self.image_model = self.image_model.to(self.device)\n",
    "                \n",
    "                # Enable memory efficient attention\n",
    "                if hasattr(self.image_model, 'enable_xformers_memory_efficient_attention'):\n",
    "                    try:\n",
    "                        self.image_model.enable_xformers_memory_efficient_attention()\n",
    "                        logger.info(\"   ‚ö° xformers memory optimization enabled\")\n",
    "                    except:\n",
    "                        logger.info(\"   ‚ö†Ô∏è  xformers not available, using default attention\")\n",
    "                \n",
    "                # Enable model CPU offload for memory efficiency\n",
    "                if hasattr(self.image_model, 'enable_model_cpu_offload'):\n",
    "                    self.image_model.enable_model_cpu_offload()\n",
    "                    logger.info(\"   üíæ CPU offload enabled for memory efficiency\")\n",
    "                \n",
    "                load_time = time.time() - start_time\n",
    "                self.stats['models_loaded'] += 1\n",
    "                logger.info(f\"‚úÖ Image model loaded in {load_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to load image model: {e}\")\n",
    "                self.stats['errors'] += 1\n",
    "                raise\n",
    "    \n",
    "    async def load_embedding_model(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Load embedding model\"\"\"\n",
    "        if self.embedding_model is None:\n",
    "            logger.info(f\"üîó Loading embedding model: {model_name}\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            \n",
    "            try:\n",
    "                self.embedding_model = SentenceTransformer(model_name)\n",
    "                self.embedding_model.to(self.device)\n",
    "                \n",
    "                load_time = time.time() - start_time\n",
    "                self.stats['models_loaded'] += 1\n",
    "                logger.info(f\"‚úÖ Embedding model loaded in {load_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to load embedding model: {e}\")\n",
    "                self.stats['errors'] += 1\n",
    "                raise\n",
    "    \n",
    "    async def process_text(self, text: str, model: str = \"default\", \n",
    "                          parameters: Optional[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced text processing with Sun Colab optimizations\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            await self.load_text_model()\n",
    "            \n",
    "            # Enhanced parameters\n",
    "            params = parameters or {}\n",
    "            max_length = min(params.get('max_length', 150), 512)  # Prevent memory issues\n",
    "            temperature = params.get('temperature', 0.8)\n",
    "            top_p = params.get('top_p', 0.9)\n",
    "            do_sample = params.get('do_sample', True)\n",
    "            \n",
    "            # Tokenize with proper attention mask\n",
    "            inputs = self.text_tokenizer(\n",
    "                text, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                max_length=256,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate with optimizations\n",
    "            with torch.no_grad():\n",
    "                outputs = self.text_model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_length=max_length,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    do_sample=do_sample,\n",
    "                    pad_token_id=self.text_tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode only the new tokens\n",
    "            generated_text = self.text_tokenizer.decode(\n",
    "                outputs[0][inputs.input_ids.shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.stats['tasks_processed'] += 1\n",
    "            self.stats['total_processing_time'] += processing_time\n",
    "            \n",
    "            return {\n",
    "                'generated_text': text + generated_text,\n",
    "                'new_text': generated_text,\n",
    "                'model': model,\n",
    "                'parameters': params,\n",
    "                'processing_time': processing_time,\n",
    "                'session_id': self.session_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['errors'] += 1\n",
    "            logger.error(f\"Text processing error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def generate_image(self, prompt: str, style: Optional[str] = None,\n",
    "                           size: str = \"512x512\", **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced image generation with Sun Colab optimizations\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            await self.load_image_model()\n",
    "            \n",
    "            # Parse size and limit for memory\n",
    "            width, height = map(int, size.split('x'))\n",
    "            width = min(width, 1024)\n",
    "            height = min(height, 1024)\n",
    "            \n",
    "            # Apply style enhancements\n",
    "            enhanced_prompt = prompt\n",
    "            if style:\n",
    "                style_enhancements = {\n",
    "                    'anime': ', anime style, manga, cel shading, vibrant colors',\n",
    "                    'realistic': ', photorealistic, 8k uhd, professional photography, detailed',\n",
    "                    'artistic': ', oil painting, fine art, masterpiece, detailed brushwork',\n",
    "                    'cyberpunk': ', cyberpunk style, neon lights, futuristic, high tech',\n",
    "                    'fantasy': ', fantasy art, magical, ethereal, mystical atmosphere',\n",
    "                    'portrait': ', portrait photography, professional lighting, detailed face',\n",
    "                    'landscape': ', landscape photography, wide angle, natural lighting'\n",
    "                }\n",
    "                enhanced_prompt += style_enhancements.get(style, '')\n",
    "            \n",
    "            # Optimize generation parameters\n",
    "            num_steps = min(kwargs.get('steps', 25), 50)  # Limit steps for speed\n",
    "            guidance_scale = kwargs.get('guidance_scale', 7.5)\n",
    "            \n",
    "            # Generate with memory management\n",
    "            with torch.autocast(device_type='cuda' if self.device.type == 'cuda' else 'cpu'):\n",
    "                image = self.image_model(\n",
    "                    enhanced_prompt,\n",
    "                    height=height,\n",
    "                    width=width,\n",
    "                    num_inference_steps=num_steps,\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    negative_prompt=\"blurry, low quality, distorted, deformed\"\n",
    "                ).images[0]\n",
    "            \n",
    "            # Convert to base64\n",
    "            buffered = io.BytesIO()\n",
    "            image.save(buffered, format=\"PNG\", optimize=True)\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.stats['tasks_processed'] += 1\n",
    "            self.stats['total_processing_time'] += processing_time\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            return {\n",
    "                'image': img_base64,\n",
    "                'prompt': enhanced_prompt,\n",
    "                'original_prompt': prompt,\n",
    "                'size': f\"{width}x{height}\",\n",
    "                'style': style,\n",
    "                'steps': num_steps,\n",
    "                'processing_time': processing_time,\n",
    "                'session_id': self.session_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['errors'] += 1\n",
    "            logger.error(f\"Image generation error: {e}\")\n",
    "            # Clear GPU cache on error\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            raise\n",
    "    \n",
    "    async def generate_embeddings(self, texts: List[str], \n",
    "                                model: str = \"sentence-transformers\") -> Dict[str, Any]:\n",
    "        \"\"\"Batch embedding generation with optimization\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            await self.load_embedding_model()\n",
    "            \n",
    "            # Batch processing for efficiency\n",
    "            batch_size = min(len(texts), 32)  # Optimal batch size\n",
    "            \n",
    "            embeddings = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = self.embedding_model.encode(\n",
    "                        batch,\n",
    "                        convert_to_tensor=True,\n",
    "                        device=self.device,\n",
    "                        show_progress_bar=False,\n",
    "                        normalize_embeddings=True\n",
    "                    )\n",
    "                    embeddings.append(batch_embeddings.cpu().numpy())\n",
    "            \n",
    "            # Concatenate all batches\n",
    "            final_embeddings = np.vstack(embeddings)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.stats['tasks_processed'] += 1\n",
    "            self.stats['total_processing_time'] += processing_time\n",
    "            \n",
    "            return {\n",
    "                'embeddings': final_embeddings.tolist(),\n",
    "                'shape': final_embeddings.shape,\n",
    "                'model': model,\n",
    "                'processing_time': processing_time,\n",
    "                'session_id': self.session_id\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.stats['errors'] += 1\n",
    "            logger.error(f\"Embedding generation error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_system_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive system statistics\"\"\"\n",
    "        import psutil\n",
    "        \n",
    "        # GPU stats\n",
    "        gpu_stats = {}\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_stats = {\n",
    "                'name': torch.cuda.get_device_name(0),\n",
    "                'memory_allocated': torch.cuda.memory_allocated(0) / 1e9,\n",
    "                'memory_total': torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "                'utilization': torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0\n",
    "            }\n",
    "        \n",
    "        # CPU and memory stats\n",
    "        memory = psutil.virtual_memory()\n",
    "        \n",
    "        return {\n",
    "            'session_info': {\n",
    "                'session_id': self.session_id,\n",
    "                'start_time': self.stats['session_start'].isoformat(),\n",
    "                'uptime_minutes': (datetime.now() - self.stats['session_start']).total_seconds() / 60\n",
    "            },\n",
    "            'processing_stats': self.stats,\n",
    "            'models_loaded': {\n",
    "                'text': self.text_model is not None,\n",
    "                'image': self.image_model is not None,\n",
    "                'embedding': self.embedding_model is not None\n",
    "            },\n",
    "            'system_resources': {\n",
    "                'cpu_percent': psutil.cpu_percent(),\n",
    "                'memory_percent': memory.percent,\n",
    "                'memory_available_gb': memory.available / 1e9,\n",
    "                'gpu': gpu_stats\n",
    "            },\n",
    "            'device': str(self.device),\n",
    "            'authenticated': bool(self.api_key)\n",
    "        }\n",
    "\n",
    "# Initialize the Sun Colab processor\n",
    "processor = SunColabProcessor()\n",
    "print(f\"\\n‚úÖ Sun Colab Processor initialized successfully!\")\n",
    "print(f\"   Session ID: {processor.session_id}\")\n",
    "print(f\"   Device: {processor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api-server"
   },
   "source": [
    "## üåê FastAPI Server with Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fastapi-server"
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends, Header\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Dict, Any\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Allow nested asyncio for Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Security\n",
    "security = HTTPBearer()\n",
    "\n",
    "# Request models\n",
    "class ProcessingRequest(BaseModel):\n",
    "    task_id: str\n",
    "    task_type: str\n",
    "    payload: Dict[str, Any]\n",
    "    priority: int = 5\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "    model: str = \"default\"\n",
    "    parameters: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class ImageRequest(BaseModel):\n",
    "    prompt: str\n",
    "    style: Optional[str] = None\n",
    "    size: str = \"512x512\"\n",
    "    steps: int = 25\n",
    "    guidance_scale: float = 7.5\n",
    "\n",
    "class EmbeddingRequest(BaseModel):\n",
    "    texts: List[str]\n",
    "    model: str = \"sentence-transformers\"\n",
    "\n",
    "# Authentication\n",
    "def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    \"\"\"Verify the API token\"\"\"\n",
    "    if not processor.authenticate_request(credentials.credentials):\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid authentication token\")\n",
    "    return credentials.credentials\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Sun Colab AI Processing Server\",\n",
    "    version=\"1.0.0\",\n",
    "    description=\"Secure GPU-accelerated AI processing with Sun Colab\"\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Public endpoint with basic info\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Sun Colab AI Processing Server\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"session_id\": processor.session_id,\n",
    "        \"capabilities\": [\n",
    "            \"text_generation\",\n",
    "            \"image_generation\", \n",
    "            \"embeddings\",\n",
    "            \"batch_processing\"\n",
    "        ],\n",
    "        \"authentication\": \"required\",\n",
    "        \"gpu_available\": torch.cuda.is_available()\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Public health check\"\"\"\n",
    "    stats = processor.get_system_stats()\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"session_id\": processor.session_id,\n",
    "        \"uptime_minutes\": stats['session_info']['uptime_minutes'],\n",
    "        \"load\": stats['system_resources']['cpu_percent'] / 100,\n",
    "        \"memory_available_gb\": stats['system_resources']['memory_available_gb'],\n",
    "        \"gpu_memory_gb\": stats['system_resources']['gpu'].get('memory_total', 0),\n",
    "        \"models_loaded\": stats['models_loaded'],\n",
    "        \"tasks_processed\": stats['processing_stats']['tasks_processed']\n",
    "    }\n",
    "\n",
    "@app.post(\"/process\")\n",
    "async def process_task(request: ProcessingRequest, token: str = Depends(verify_token)):\n",
    "    \"\"\"Main processing endpoint with authentication\"\"\"\n",
    "    try:\n",
    "        task_type = request.task_type\n",
    "        payload = request.payload\n",
    "        \n",
    "        result = None\n",
    "        \n",
    "        if task_type == \"text_generation\":\n",
    "            result = await processor.process_text(\n",
    "                payload.get('text', ''),\n",
    "                payload.get('model', 'default'),\n",
    "                payload.get('parameters', {})\n",
    "            )\n",
    "        \n",
    "        elif task_type == \"image_generation\":\n",
    "            result = await processor.generate_image(\n",
    "                payload.get('prompt', ''),\n",
    "                payload.get('style'),\n",
    "                payload.get('size', '512x512'),\n",
    "                steps=payload.get('steps', 25),\n",
    "                guidance_scale=payload.get('guidance_scale', 7.5)\n",
    "            )\n",
    "        \n",
    "        elif task_type == \"embeddings\":\n",
    "            result = await processor.generate_embeddings(\n",
    "                payload.get('texts', []),\n",
    "                payload.get('model', 'sentence-transformers')\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "        \n",
    "        return {\n",
    "            'task_id': request.task_id,\n",
    "            'status': 'completed',\n",
    "            'result': result,\n",
    "            'session_id': processor.session_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/text\", dependencies=[Depends(verify_token)])\n",
    "async def generate_text(request: TextRequest):\n",
    "    \"\"\"Direct text generation endpoint\"\"\"\n",
    "    try:\n",
    "        result = await processor.process_text(\n",
    "            request.text,\n",
    "            request.model,\n",
    "            request.parameters\n",
    "        )\n",
    "        return {\"success\": True, \"result\": result}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/image\", dependencies=[Depends(verify_token)])\n",
    "async def generate_image(request: ImageRequest):\n",
    "    \"\"\"Direct image generation endpoint\"\"\"\n",
    "    try:\n",
    "        result = await processor.generate_image(\n",
    "            request.prompt,\n",
    "            request.style,\n",
    "            request.size,\n",
    "            steps=request.steps,\n",
    "            guidance_scale=request.guidance_scale\n",
    "        )\n",
    "        return {\"success\": True, \"result\": result}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/embeddings\", dependencies=[Depends(verify_token)])\n",
    "async def generate_embeddings(request: EmbeddingRequest):\n",
    "    \"\"\"Direct embeddings generation endpoint\"\"\"\n",
    "    try:\n",
    "        result = await processor.generate_embeddings(\n",
    "            request.texts,\n",
    "            request.model\n",
    "        )\n",
    "        return {\"success\": True, \"result\": result}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/stats\", dependencies=[Depends(verify_token)])\n",
    "async def get_stats():\n",
    "    \"\"\"Get detailed system statistics\"\"\"\n",
    "    return processor.get_system_stats()\n",
    "\n",
    "def start_server(port=8080):\n",
    "    \"\"\"Start the FastAPI server\"\"\"\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
    "\n",
    "print(\"üåê Sun Colab FastAPI server configured with authentication\")\n",
    "print(f\"   API Key: {processor.api_key[:20] if processor.api_key else 'Not set'}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy"
   },
   "source": [
    "## üöÄ Deploy with ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy-server"
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Start server in background\n",
    "def run_server():\n",
    "    start_server(port=8080)\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "print(\"‚è≥ Starting server...\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Create ngrok tunnel\n",
    "try:\n",
    "    # Kill any existing tunnels\n",
    "    ngrok.kill()\n",
    "    \n",
    "    # Create new tunnel\n",
    "    public_url = ngrok.connect(8080)\n",
    "    \n",
    "    print(f\"\\nüåü Sun Colab Server Successfully Deployed!\")\n",
    "    print(f\"\\nüìã Server Details:\")\n",
    "    print(f\"   Public URL: {public_url}\")\n",
    "    print(f\"   Session ID: {processor.session_id}\")\n",
    "    print(f\"   API Key: {processor.api_key[:20]}...\")\n",
    "    \n",
    "    print(f\"\\nüîß Flask Integration:\")\n",
    "    print(f\"   Add to your .env file:\")\n",
    "    print(f\"   COLAB_PROCESSING_URLS={public_url}\")\n",
    "    print(f\"   SUN_COLAB_KEY={processor.api_key}\")\n",
    "    \n",
    "    print(f\"\\nüß™ Test Commands:\")\n",
    "    print(f\"   curl -X GET {public_url}/health\")\n",
    "    print(f\"   curl -X GET {public_url}/ \")\n",
    "    \n",
    "    # Test the server\n",
    "    try:\n",
    "        health_response = requests.get(f\"{public_url}/health\", timeout=10)\n",
    "        if health_response.status_code == 200:\n",
    "            health_data = health_response.json()\n",
    "            print(f\"\\n‚úÖ Server Health Check Passed:\")\n",
    "            print(f\"   Status: {health_data['status']}\")\n",
    "            print(f\"   GPU Memory: {health_data.get('gpu_memory_gb', 0):.1f} GB\")\n",
    "            print(f\"   Models Ready: {sum(health_data['models_loaded'].values())}/3\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Health check failed: {health_response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Health check error: {e}\")\n",
    "    \n",
    "    print(f\"\\nüîí Security Features:\")\n",
    "    print(f\"   ‚úÖ Bearer token authentication\")\n",
    "    print(f\"   ‚úÖ Colab secrets integration\")\n",
    "    print(f\"   ‚úÖ Request validation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to create ngrok tunnel: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Make sure ngrok is installed\")\n",
    "    print(\"   2. Check if port 8080 is available\")\n",
    "    print(\"   3. Restart the runtime if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-endpoints"
   },
   "source": [
    "## üß™ Test Processing Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-processing"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"üß™ Testing Sun Colab Processing Endpoints\\n\")\n",
    "\n",
    "# Get the API endpoint and key\n",
    "api_url = str(public_url)\n",
    "api_key = processor.api_key\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Test 1: Text Generation\n",
    "print(\"1Ô∏è‚É£ Testing Text Generation...\")\n",
    "try:\n",
    "    text_data = {\n",
    "        \"text\": \"The future of artificial intelligence is\",\n",
    "        \"parameters\": {\n",
    "            \"max_length\": 100,\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{api_url}/text\", json=text_data, headers=headers, timeout=30)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        generated = result['result']['new_text'][:100]\n",
    "        processing_time = result['result']['processing_time']\n",
    "        print(f\"   ‚úÖ Generated: '{generated}...'\")\n",
    "        print(f\"   ‚è±Ô∏è  Processing time: {processing_time:.2f}s\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {response.status_code} - {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ùå Exception: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test 2: Image Generation\n",
    "print(\"2Ô∏è‚É£ Testing Image Generation...\")\n",
    "try:\n",
    "    image_data = {\n",
    "        \"prompt\": \"a beautiful sunset over mountains, digital art\",\n",
    "        \"style\": \"artistic\",\n",
    "        \"size\": \"512x512\",\n",
    "        \"steps\": 20\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{api_url}/image\", json=image_data, headers=headers, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        processing_time = result['result']['processing_time']\n",
    "        image_b64 = result['result']['image']\n",
    "        \n",
    "        print(f\"   ‚úÖ Image generated successfully\")\n",
    "        print(f\"   ‚è±Ô∏è  Processing time: {processing_time:.2f}s\")\n",
    "        print(f\"   üìê Size: {result['result']['size']}\")\n",
    "        print(f\"   üé® Style: {result['result']['style']}\")\n",
    "        \n",
    "        # Display the image\n",
    "        try:\n",
    "            image_data = base64.b64decode(image_b64)\n",
    "            display(Image(data=image_data, width=300))\n",
    "        except:\n",
    "            print(\"   ‚ÑπÔ∏è  Image generated but display failed (normal in some environments)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {response.status_code} - {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ùå Exception: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test 3: Embeddings\n",
    "print(\"3Ô∏è‚É£ Testing Embeddings Generation...\")\n",
    "try:\n",
    "    embedding_data = {\n",
    "        \"texts\": [\n",
    "            \"Machine learning is revolutionizing technology\",\n",
    "            \"Artificial intelligence will transform industries\",\n",
    "            \"Deep learning models are becoming more powerful\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{api_url}/embeddings\", json=embedding_data, headers=headers, timeout=30)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        embeddings = result['result']['embeddings']\n",
    "        shape = result['result']['shape']\n",
    "        processing_time = result['result']['processing_time']\n",
    "        \n",
    "        print(f\"   ‚úÖ Embeddings generated successfully\")\n",
    "        print(f\"   üìä Shape: {shape}\")\n",
    "        print(f\"   ‚è±Ô∏è  Processing time: {processing_time:.2f}s\")\n",
    "        \n",
    "        # Calculate similarity between first two texts\n",
    "        import numpy as np\n",
    "        emb1 = np.array(embeddings[0])\n",
    "        emb2 = np.array(embeddings[1])\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        print(f\"   üîó Similarity (text 1-2): {similarity:.3f}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {response.status_code} - {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ùå Exception: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test 4: System Stats\n",
    "print(\"4Ô∏è‚É£ Testing System Statistics...\")\n",
    "try:\n",
    "    response = requests.get(f\"{api_url}/stats\", headers=headers, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        stats = response.json()\n",
    "        \n",
    "        print(f\"   ‚úÖ Stats retrieved successfully\")\n",
    "        print(f\"   üñ•Ô∏è  Session ID: {stats['session_info']['session_id']}\")\n",
    "        print(f\"   ‚è∞ Uptime: {stats['session_info']['uptime_minutes']:.1f} minutes\")\n",
    "        print(f\"   üìä Tasks processed: {stats['processing_stats']['tasks_processed']}\")\n",
    "        print(f\"   üîß Models loaded: {sum(stats['models_loaded'].values())}/3\")\n",
    "        print(f\"   üíæ Memory usage: {stats['system_resources']['memory_percent']:.1f}%\")\n",
    "        \n",
    "        if stats['system_resources']['gpu']:\n",
    "            gpu = stats['system_resources']['gpu']\n",
    "            print(f\"   üéÆ GPU: {gpu.get('name', 'Unknown')}\")\n",
    "            print(f\"   üéÆ GPU Memory: {gpu.get('memory_allocated', 0):.1f}/{gpu.get('memory_total', 0):.1f} GB\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Error: {response.status_code} - {response.text}\")\nexcept Exception as e:\n",
    "    print(f\"   ‚ùå Exception: {e}\")\n",
    "\n",
    "print(\"\\nüéâ All tests completed!\")\n",
    "print(f\"\\nüìã Integration Summary:\")\n",
    "print(f\"   Server URL: {api_url}\")\n",
    "print(f\"   Session ID: {processor.session_id}\")\n",
    "print(f\"   Authentication: ‚úÖ Active\")\n",
    "print(f\"   GPU Processing: ‚úÖ Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keep-alive"
   },
   "source": [
    "## üîÑ Keep Server Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor-server"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üåü Sun Colab Server is running!\")\n",
    "print(f\"\\nüìã Connection Details:\")\n",
    "print(f\"   URL: {public_url}\")\n",
    "print(f\"   API Key: {processor.api_key}\")\n",
    "print(f\"   Session: {processor.session_id}\")\n",
    "\n",
    "print(f\"\\nüîß Add to your Flask .env:\")\n",
    "print(f\"   COLAB_PROCESSING_URLS={public_url}\")\n",
    "print(f\"   SUN_COLAB_KEY={processor.api_key}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server!\")\n",
    "print(f\"\\nüìä Real-time monitoring:\")\n",
    "\n",
    "# Monitoring loop\n",
    "start_time = datetime.now()\n",
    "check_count = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(30)  # Check every 30 seconds\n",
    "        check_count += 1\n",
    "        \n",
    "        try:\n",
    "            # Health check\n",
    "            health = requests.get(f\"{public_url}/health\", timeout=5)\n",
    "            \n",
    "            if health.status_code == 200:\n",
    "                data = health.json()\n",
    "                uptime = (datetime.now() - start_time).total_seconds() / 60\n",
    "                \n",
    "                status_line = (\n",
    "                    f\"\\r[{datetime.now().strftime('%H:%M:%S')}] \"\n",
    "                    f\"‚úÖ Healthy | \"\n",
    "                    f\"Uptime: {uptime:.1f}m | \"\n",
    "                    f\"Tasks: {data.get('tasks_processed', 0)} | \"\n",
    "                    f\"Memory: {data.get('memory_available_gb', 0):.1f}GB | \"\n",
    "                    f\"Checks: {check_count}\"\n",
    "                )\n",
    "                print(status_line, end='', flush=True)\n",
    "                \n",
    "                # Detailed status every 10 checks (5 minutes)\n",
    "                if check_count % 10 == 0:\n",
    "                    print(f\"\\n\\nüìä Status Update (Check #{check_count}):\")\n",
    "                    print(f\"   Uptime: {uptime:.1f} minutes\")\n",
    "                    print(f\"   Tasks Processed: {data.get('tasks_processed', 0)}\")\n",
    "                    print(f\"   GPU Memory: {data.get('gpu_memory_gb', 0):.1f} GB\")\n",
    "                    print(f\"   Models Loaded: {sum(data.get('models_loaded', {}).values())}/3\")\n",
    "                    print(f\"   Session ID: {data.get('session_id', 'Unknown')}\")\n",
    "                    print(\"\\nüì° Continuing monitoring...\")\n",
    "            else:\n",
    "                print(f\"\\r‚ùå Health check failed: {health.status_code}\", end='', flush=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\r‚ö†Ô∏è  Connection error: {str(e)[:50]}\", end='', flush=True)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n\\nüõë Monitoring stopped by user\")\n",
    "    final_uptime = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"üìä Final Statistics:\")\n",
    "    print(f\"   Total uptime: {final_uptime:.1f} minutes\")\n",
    "    print(f\"   Health checks: {check_count}\")\n",
    "    print(f\"   Session ID: {processor.session_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n‚ùå Monitoring error: {e}\")\n",
    "    print(\"Server may still be running. Check manually with health endpoint.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "mount_file_id": "1EwfBj0nC9St-2hB1bv2zGWWDTcS4slsx",
   "authorship_tag": "ABX9TyM..."
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}